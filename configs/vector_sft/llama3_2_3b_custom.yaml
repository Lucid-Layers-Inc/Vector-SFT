project:
  name: "Experiment1-again"
  experiment_name: "linear-systems"
  hf_checkpoint: "ExplosionNuclear/Experiment1-again"

clearml:
  project_name: ${project.name}
  task_name: ${project.experiment_name}
  output_uri: false

dataset:
  name: "ExplosionNuclear/ExpNew7"
  train_size: 12500
  eval_size: 2000
  max_length: 800

model:
  name: "meta-llama/Llama-3.2-3B-Instruct"
  dtype: "bfloat16"
  device_map: "cuda"
  attn_implementation: "flash_attention_2"
  special_tokens:
    - "<simple_talk>"
    - "</simple_talk>"

auxiliary:
  N_max: 300 # maximal length of mathematical proof (actually it's smaller then that)
  num_segments: 20
  beta_1: 0.3
  beta_2: 0.7
  beta_3: 2
  segments_rank: 256


peft:
  r: 128
  lora_alpha: 128
  lora_dropout: 0.05
  bias: "lora_only"
  task_type: "CAUSAL_LM"
  use_rslora: false
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
    - "o_proj"

trainer:
  output_dir: "./VectorSFT-checkpoints"
  num_train_epochs: 12
  per_device_train_batch_size: 16
  gradient_accumulation_steps: 8
  learning_rate: 3e-4
  weight_decay: 0.01
  lr_scheduler_type: "linear"
  warmup_steps: 100
  logging_steps: 10
  save_steps: 100
  bf16: true
  tf32: true
  gradient_checkpointing: false
  optim: "adamw_torch"
  report_to: "none"
  max_grad_norm: 0.3
  ddp_find_unused_parameters: false
  push_to_hub: true
  hub_model_id: ${project.hf_checkpoint}
  eval_strategy: "epoch"
  remove_unused_columns: false
  dataset_kwargs:
    skip_prepare_dataset: true
