project:
  name: "Experiment2"
  experiment_name: "linear-systems-95"
  hf_checkpoint: "ExplosionNuclear/Experiment2-95"

clearml:
  project_name: ${project.name}
  task_name: ${project.experiment_name}
  output_uri: false

dataset:
  name: "ExplosionNuclear/ExpNew7"
  train_size: 12500
  eval_size: 2000
  max_length: 800

calibration_dataset:
  name: "ExplosionNuclear/new_simple_talks"

calib_prob: 0.05

model:
  name: ExplosionNuclear/Llama-2.3-3B-Instruct-special
  dtype: "bfloat16"
  device_map: "cuda"
  attn_implementation: "flash_attention_2"

auxiliary:
  N_max: 300 
  num_segments: 20
  segments_rank: 256

betas:
  beta_1: 0.3
  beta_2: 0.7
  beta_3: 2

peft:
  r: 128
  lora_alpha: 128
  lora_dropout: 0.05
  bias: "lora_only"
  task_type: "CAUSAL_LM"
  use_rslora: false
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
    - "o_proj"

resume_from_checkpoint: null

trainer:
  output_dir: "./VectorSFT-checkpoints"
  num_train_epochs: 20
  per_device_train_batch_size: 16
  gradient_accumulation_steps: 8
  learning_rate: 3e-4
  weight_decay: 0.01
  lr_scheduler_type: "linear"
  warmup_steps: 100
  logging_steps: 10
  save_steps: 100
  bf16: true
  tf32: false
  gradient_checkpointing: false
  optim: "adamw_torch"
  report_to: "none"
  max_grad_norm: 0.3
  ddp_find_unused_parameters: false
  push_to_hub: true
  hub_model_id: ${project.hf_checkpoint}
  eval_strategy: "epoch"
  remove_unused_columns: false
  dataset_kwargs:
    skip_prepare_dataset: true