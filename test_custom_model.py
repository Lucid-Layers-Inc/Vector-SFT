#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ CustomLlamaModel –∏ CustomLlamaForCausalLM
"""

import torch
from transformers import AutoTokenizer, LlamaConfig
from src.experiments.CustomModel import CustomLlamaModel, CustomLlamaForCausalLM

def test_custom_model():
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–∞—Å—Ç–æ–º–Ω—É—é –º–æ–¥–µ–ª—å"""
    
    # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    config = LlamaConfig(
        vocab_size=32000,
        hidden_size=512,
        intermediate_size=1024,
        num_hidden_layers=4,
        num_attention_heads=8,
        max_position_embeddings=2048
    )
    
    print("–°–æ–∑–¥–∞–µ–º CustomLlamaModel...")
    model = CustomLlamaModel(config)
    print(f"‚úì CustomLlamaModel —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
    
    print("\n–°–æ–∑–¥–∞–µ–º CustomLlamaForCausalLM...")
    causal_model = CustomLlamaForCausalLM(config)
    print(f"‚úì CustomLlamaForCausalLM —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º forward pass
    batch_size, seq_length = 2, 10
    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_length))
    attention_mask = torch.ones(batch_size, seq_length)
    
    print(f"\n–¢–µ—Å—Ç–∏—Ä—É–µ–º forward pass CustomLlamaModel...")
    print(f"Input shape: {input_ids.shape}")
    
    # –¢–µ—Å—Ç –±–µ–∑ intermediate_layer_idx
    with torch.no_grad():
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=True
        )
    
    print(f"‚úì Output shape: {outputs.last_hidden_state.shape}")
    print(f"‚úì Intermediate activations: {outputs.intermediate_activations}")
    
    # –¢–µ—Å—Ç —Å intermediate_layer_idx
    with torch.no_grad():
        outputs_with_intermediate = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            intermediate_layer_idx=1,  # –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —Å–æ 2-–≥–æ —Å–ª–æ—è (–∏–Ω–¥–µ–∫—Å 1)
            return_dict=True
        )
    
    print(f"‚úì Output with intermediate (layer 1): {outputs_with_intermediate.intermediate_activations.shape}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º CustomLlamaForCausalLM
    print(f"\n–¢–µ—Å—Ç–∏—Ä—É–µ–º forward pass CustomLlamaForCausalLM...")
    with torch.no_grad():
        causal_outputs = causal_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            intermediate_layer_idx=2,
            return_dict=True
        )
    
    print(f"‚úì Logits shape: {causal_outputs.logits.shape}")
    print(f"‚úì Intermediate activations shape: {causal_outputs.intermediate_activations.shape}")
    
    # –¢–µ—Å—Ç —Å labels –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ loss
    labels = input_ids.clone()
    with torch.no_grad():
        causal_outputs_with_loss = causal_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels,
            return_dict=True
        )
    
    print(f"‚úì Loss: {causal_outputs_with_loss.loss.item():.4f}")
    
    print(f"\nüéâ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏ —É—Å–ø–µ—à–Ω–æ!")
    

if __name__ == "__main__":
    test_custom_model() 